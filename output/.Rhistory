fit <- lm(avgval~hupc,data=countyTab)
summary(fit)
fit <- lm(log(avgval)~log(hupc),data=countyTab)
summary(fit)
fit <- lm(log(avgval)~hupc,data=countyTab)
summary(fit)
fit <- lm(avgval~log(hupc),data=countyTab)
summary(fit)
fit <- lm(log(avgval)~COHU+COPOP,data=countyTab)
summary(fit)
fit <- lm(log(avgval)~log(COHU)+log(COPOP),data=countyTab)
summary(fit)
tAHS2011$BATHS
tAHS2011$BATHS[which(tAHS2011$BATHS<0)] <- NA
describe(tAHS2011$BATHS)
describe(tAHS2011$BEDRMS)
describe(tAHS2011$UNITSF)
tAHS2011$UNITSF[which(tAHS2011$UNITSF<0)] <- NA
countyTab <- data.table(tAHS2011)[,.(
hhs=sum(!is.na(CONTROL))
,avgval=weighted.mean(VALUE,WEIGHT,na.rm=TRUE)
,avgvalpsf=weighted.mean(VALUE/UNITSF,WEIGHT,na.rm=TRUE)
,avgfh=weighted.mean(FIRSTHOME,WEIGHT,na.rm=TRUE)
,avgdp=weighted.mean(DOWNPERCENT,WEIGHT,na.rm=TRUE)
),by=.(STATE,COUNTY)]
describe(countyTab$avgvalpsf)
tAHS2011$valpersf <- tAHS2011$VALUE/tAHS2011$UNITSF
countyTab <- data.table(tAHS2011)[,.(
hhs=sum(!is.na(CONTROL))
,avgval=weighted.mean(VALUE,WEIGHT,na.rm=TRUE)
,avgvalpsf=weighted.mean(valpersf,WEIGHT,na.rm=TRUE)
,avgfh=weighted.mean(FIRSTHOME,WEIGHT,na.rm=TRUE)
,avgdp=weighted.mean(DOWNPERCENT,WEIGHT,na.rm=TRUE)
),by=.(STATE,COUNTY)]
fit <- lm(valpersf~BATHS+BEDRMS,data=tAHS2011)
summary(fit)
keep <- c("valpersf","VALUE",'BEDRMS',"BATHS","UNITSF","STATE","COUNTY")
hsSlim <- tAHS2011[keep]
hsSlim <- hsSlim[complete.cases(hsSlim),]
hsSlim$STATE <- as.integer(hsSlim$STATE)
hsSlim <- merge(hsSlim,coStats,by=c("STATE","COUNTY"),all.x=TRUE)
names(hsSlim)
fit <- lm(valpersf~BEDRMS+BATHS+COAREALAND+COHU+COPOP,data=hsSlim)
summary(fit)
fit <- lm(log(VALUE)~log(UNITSF)+BEDRMS+BATHS+log(COAREALAND)+log(COHU)+log(COPOP),data=hsSlim)
summary(fit)
fit <- lm(log(VALUE)~UNITSF+BEDRMS+BATHS+COAREALAND+COHU+COPOP,data=hsSlim)
summary(fit)
fit <- lm(log(VALUE)~log(UNITSF)+BEDRMS+BATHS+log(COAREALAND)+log(COHU)+log(COPOP),data=hsSlim)
summary(fit)
commuters <- read.csv("~/Data/US data/county_to_county.csv",na.strings="")
View(commuters)
?read.csv
names(commuters)
commuters <- read.csv("~/Data/US data/county_to_county.csv"
,na.strings=""
,colClasses=c("integer","character","integer","integer"))
commuters <- read.csv("~/Data/US data/county_to_county.csv"
,na.strings=""
,colClasses=c("numeric","character","numeric","numeric"))
commuters <- read.csv("~/Data/US data/county_to_county.csv"
,na.strings=""
,colClasses=c("numeric","character","numeric","numeric"))
View(commuters)
commuters <- read.csv("~/Data/US data/county_to_county.csv"
,na.strings=""
,colClasses=c("numeric","character","numeric","numeric"))
View(commuters)
hsSlim <- merge(hsSlim,commuters,by=c("STATE","COUNTY"),all.x=TRUE)
names(commuters)
keep <- c("valpersf","VALUE",'BEDRMS',"BATHS","UNITSF","STATE","COUNTY")
hsSlim <- tAHS2011[keep]
hsSlim <- hsSlim[complete.cases(hsSlim),]
hsSlim$STATE <- as.integer(hsSlim$STATE)
coStats <- zcta_county_rel_10[
c("STATE","COUNTY","COAREALAND","COHU","COPOP")
]
coStats <- unique(coStats)
hsSlim <- merge(hsSlim,coStats,by=c("STATE","COUNTY"),all.x=TRUE)
fit <- lm(log(VALUE)~log(UNITSF)+BEDRMS+BATHS+log(COAREALAND)+log(COHU)+log(COPOP),data=hsSlim)
summary(fit)
commuters <- read.csv("~/Data/US data/county_to_county.csv"
,na.strings=""
,colClasses=c("numeric","character","numeric","numeric"))
commuters <- data.table(commuters)[,.(COMMUTERS=sum(COMMUTERS,na.rm=TRUE)),by=.(STATE,COUNTY)]
hsSlim <- merge(hsSlim,commuters,by=c("STATE","COUNTY"),all.x=TRUE)
keep <- c("valpersf","VALUE",'BEDRMS',"BATHS","UNITSF","STATE","COUNTY")
hsSlim <- tAHS2011[keep]
hsSlim <- hsSlim[complete.cases(hsSlim),]
hsSlim$STATE <- as.integer(hsSlim$STATE)
coStats <- zcta_county_rel_10[
c("STATE","COUNTY","COAREALAND","COHU","COPOP")
]
coStats <- unique(coStats)
hsSlim <- merge(hsSlim,coStats,by=c("STATE","COUNTY"),all.x=TRUE)
fit <- lm(log(VALUE)~log(UNITSF)+BEDRMS+BATHS+log(COAREALAND)+log(COHU)+log(COPOP),data=hsSlim)
summary(fit)
commuters <- read.csv("~/Data/US data/county_to_county.csv"
,na.strings=""
,colClasses=c("numeric","character","numeric","numeric"))
commuters <- data.table(commuters)[,.(COMMUTERS=sum(COMMUTERS,na.rm=TRUE)),by=.(STATE,COUNTY)]
hsSlim <- merge(hsSlim,commuters,by=c("STATE","COUNTY"),all.x=TRUE)
describe(hsSlim$COMMUTERS)
fit <- lm(log(VALUE)~log(UNITSF)+BEDRMS+BATHS+log(COAREALAND)+log(COHU)+log(COPOP)+log(COMMUTERS),data=hsSlim)
summary(fit)
commuters <- read.csv("~/Data/US data/commuters_out.csv"
,na.strings=""
,colClasses=c("numeric","character","numeric","numeric"))
commuters <- read.csv("~/Data/US data/commuters_from.csv"
,na.strings=""
,colClasses=c("numeric","character","numeric","numeric"))
View(commuters)
commutersIn <- read.csv("~/Data/US data/commuters_to.csv"
,na.strings=""
,colClasses=c("numeric","character","numeric"))
View(commutersIn)
View(commutersIn)
commutersIn <- data.table(commutersIn)[,.(COMMUTERS_IN=sum(COMMUTERS_IN,na.rm=TRUE)),by=.(STATE,COUNTY)]
commuters <- read.csv("~/Data/US data/commuters_from.csv"
,na.strings=""
,colClasses=c("numeric","character","numeric","numeric"))
commuters <- data.table(commuters)[,.(COMMUTERS=sum(COMMUTERS,na.rm=TRUE)),by=.(STATE,COUNTY)]
commutersIn <- read.csv("~/Data/US data/commuters_to.csv"
,na.strings=""
,colClasses=c("numeric","character","numeric"))
commutersIn <- data.table(commutersIn)[,.(COMMUTERS_IN=sum(COMMUTERS_IN,na.rm=TRUE)),by=.(STATE,COUNTY)]
View(commuters)
View(commutersIn)
hsSlim <- merge(hsSlim,commutersIn,by=c("STATE","COUNTY"),all.x=TRUE)
View(hsSlim)
fit <- lm(log(VALUE)~log(UNITSF)+BEDRMS+BATHS+log(COAREALAND)+log(COHU)+log(COPOP)+log(COMMUTERS)+log(COMMUTERS_IN),data=hsSlim)
summary(fit)
fit <- lm(log(VALUE)~log(UNITSF)+BEDRMS+BATHS+log(COAREALAND)+log(COHU)+log(COPOP)+log(COMMUTERS),data=hsSlim)
summary(fit)
hsSlim$statecounty <- paste0(hsSlim$STATE,hsSlim$COUNTY)
View(hsSlim)
hsSlim$statecounty <- paste(hsSlim$STATE,hsSlim$COUNTY,sep=".")
View(hsSlim)
hsSlim$statecounty <- factor(hsSlim$statecounty)
fit <- lm(log(VALUE)~log(UNITSF)+BEDRMS+BATHS+log(COAREALAND)+log(COHU)+log(COPOP)+log(COMMUTERS)+statecounty,data=hsSlim)
summary(fit)
hsSlim$STATE <- factor(hsSlim$STATE)
fit <- lm(log(VALUE)~log(UNITSF)+BEDRMS+BATHS+log(COAREALAND)+log(COHU)+log(COPOP)+log(COMMUTERS)+statecounty+STATE,data=hsSlim)
summary(fit)
fit <- lm(log(VALUE)~log(UNITSF)+BEDRMS+BATHS+log(COAREALAND)+log(COHU)+log(COPOP)+log(COMMUTERS)+statecounty,data=hsSlim)
summary(fit)
fit <- lm(log(VALUE)~log(UNITSF)+BEDRMS+BATHS+log(COAREALAND)+log(COHU)+log(COPOP)+log(COMMUTERS),data=hsSlim)
summary(fit)
coStats <- merge(coStats,commuters,by=c("STATE","COUNTY"),all=TRUE)
describe(coStats)
summary(fit)
coStats <- transform(coStats,
priceVariance = -0.131326*log(COAREALAND) + -2.404489*log(COHU) + 0.444121*log(COPOP) + 2.135840*log(COMMUTERS)
)
describe(coStats$priceVariance)
View(zcta_county_rel_10)
dmv <- subset(coStats,STATE %in% c(11,24,51))
View(dmv)
zips <- zcta_county_rel_10[c("STATE","COUNTY","ZCTA5")]
zips$STATE <- as.integer(zips$STATE)
dmv <- merge(dmv,zips,all.x=TRUE)
View(dmv)
dmv <- dmv[order(dmv$priceVariance),]
dmv <- dmv[order(dmv$priceVariance),]
dmv <- dmv[order(dmv$priceVariance),]
View(dmv)
View(zcta_county_rel_10)
library(mapdist)
install.packages("mapdist")
library(mapdist)
library(ggmap)
install.packages("ggmap")
typeof(zips$ZCTA5)
?mapdist
mapdist("20001","22044",mode="driving")
library(ggmap)
mapdist("20001","22044",mode="driving")
dat <- mapdist("20001","22044",mode="driving")
dat
rbind(dat,dat)
fromZip = "20001"
for(zip in dmv$ZCTA5){
message(zip)
}
zip <- "22044"
distList <- list()
distIndex <- 1
fromZip = "20001"
distDat <- mapdist(fromZip,zip,mode="driving")
distDat
distList[[distIndex]] <- distDat
distIndex <- distIndex + 1
distData <- rbindlist(distList)
distData
View(distData)
distList <- list()
distIndex <- 1
fromZip = "20001"
library(ggmap)
for(zip in dmv$ZCTA5){
distDat <- mapdist(fromZip,zip,mode="driving")
distList[[distIndex]] <- distDat
distIndex <- distIndex + 1
}
distData <- rbindlist(distList)
save(distData,file="~/Data/US data/distData.RData")
distList[6]
distList[33]
distData <- rbindlist(distList,fill=TRUE)
save(distData,file="~/Data/US data/distData.RData")
View(distData)
names(distData)
names(distData)["to"]
names(distData)[2]
names(distData)[2] = "ZCTA5"
distSlim <- distdata[c("ZCTA5","m")]
distSlim <- distData[c("ZCTA5","m")]
distSlim <- data.frame(distData)[c("ZCTA5","m")]
distSlim
dmv <- merge(dmv,distSlim,all.x=TRUE)
View(dmv)
lowPrice <- subset(dmv,priceVariance<0)
View(lowPrice)
names(distData)
dmv <- subset(coStats,STATE %in% c(11,24,51))
zips <- zcta_county_rel_10[c("STATE","COUNTY","ZCTA5")]
zips$STATE <- as.integer(zips$STATE)
dmv <- merge(dmv,zips,all.x=TRUE)
dmv <- dmv[order(dmv$priceVariance),]
names(distData)[2] = "ZCTA5"
dmv <- merge(dmv,distData,all.x=TRUE)
lowPrice <- subset(dmv,priceVariance<0)
View(lowPrice)
4954/(60*60)
dmv$varByHours = dmv$priceVariance/dmv$hours
close <- subset(dmv,hours<=.5)
View(close)
close <- subset(dmv,hours<=1)
View(close)
close <- subset(dmv,hours<=.75)
write.csv(dmv,"~/Data/US data/dmv.csv",na="",row.names=FALSE)
library(readr)
library(data.table)
library(Hmisc)
# tAHS2011 <- read_csv("~/Data/US data/AHS 2011 National and Metropolitan PUF v1.4 Flat CSV/tAHS2011.csv",quote="'")
# zcta_county_rel_10 <- read_csv("~/Data/US data/zcta_county_rel_10.txt")
#
# save(tAHS2011,zcta_county_rel_10,zips,file="~/Data/US data/AHS 2011 National and Metropolitan PUF v1.4 Flat CSV/tAHS2011.RData")
load("~/Data/US data/AHS 2011 National and Metropolitan PUF v1.4 Flat CSV/tAHS2011.RData")
View(zcta_county_rel_10)
commuters <- read.csv("~/Data/US data/commuters_from.csv"
,na.strings=""
,colClasses=c("numeric","character","numeric","numeric"))
commuters <- data.table(commuters)[,.(COMMUTERS=sum(COMMUTERS,na.rm=TRUE)),by=.(STATE,COUNTY)]
View(commuters)
zStats <- zcta_county_rel_10[c("STATE","COUNTY","ZCTA5","ZHU","ZPOP","ZAREALAND")]
warnings()
View(zStats)
zStats <- transform(zStats,
priceVariance = -0.131326*log(ZAREALAND) + -2.404489*log(ZHU) + 0.444121*log(ZPOP)
)
View(zStats)
load("~/Data/US data/distData.RData")
names(distData)[2] = "ZCTA5"
zStats <- merge(zStats,distData,by="ZCTA5")
View(zStats)
zStats <- subset(zStats,is.finite(priceVariance))
close_zips <- subset(zStats,minutes<=60)
View(close_zips)
?merge
library(plyr)
?mergeall
library(reshape)
?merge_all
install.packages("gdata")
library(gdata)
?read.xls
url <- "https://www.imf.org/external/pubs/ft/weo/2017/02/weodata/WEOOct2017all.xls"
perl <- "C:/Strawberry/perl/bin/perl.exe"
dat <- read.xls(url,sheet=1,na.strings="n/a",perl=perl)
url <- "https://www.imf.org/external/pubs/ft/weo/2017/02/weodata/WEOOct2017all.xls"
perl <- "C:/Strawberry/perl/bin/perl.exe"
temp <- tempfile()
download.file(url,temp)
dat <- read.xls(temp,sheet=1,na.strings="n/a",perl=perl)
?tempfile
temp <- tempfile(fileext=".xls")
download.file(url,temp)
dat <- read.xls(temp,sheet=1,na.strings="n/a",perl=perl)
temp
url <- "https://www.imf.org/external/pubs/ft/weo/2017/02/weodata/WEOOct2017all.xls"
perl <- "C:/Strawberry/perl/bin/perl.exe"
temp <- tempfile(fileext=".xlsx")
download.file(url,temp)
url <- "https://www.imf.org/external/pubs/ft/weo/2017/02/weodata/WEOOct2017all.xls"
perl <- "C:/Strawberry/perl/bin/perl.exe"
temp <- tempfile(fileext=".csv")
download.file(url,temp)
?read.delim
dat <- read.table(temp,header=TRUE,sep="\t",na.strings="n/a")
dat <- read.table(temp,header=TRUE,sep="\t",na.strings="n/a",fill=TRUE)
View(dat)
""
dat <- read.table(temp,header=TRUE,sep="\t",na.strings=c("n/a",""),fill=TRUE)
View(dat)
unique(dat$WEO.Subject.Code)
ngd <- subset(dat,WEO.Subject.Code=="NGDPRPPPPC")
names(ngd)
keep <- c(2,10:52)
ngd <- ngd[keep]
View(ngd)
unique(ngd$ISO)
unique(dat$ISO)
weird <- subset(dat,ISO=="187.446")
View(weird)
View(dat)
dat <- read.table(temp,header=TRUE,sep="\t",na.strings=c("n/a",""),fill=TRUE,strip.white=TRUE)
unique(dat$ISO)
?read_csv
library(readr)
?read_csv
dat = read_csv(temp,delim="\t")
dat = read_delim(temp,delim="\t")
View(dat)
unique(dat$ISO)
ngd <- subset(dat,WEO.Subject.Code=="NGDPRPPPPC")
names(dat)
make.names(names(dat))
names(dat) <- make.names(names(dat))
ngd <- subset(dat,WEO.Subject.Code=="NGDPRPPPPC")
View(ngd)
keep <- c(2,10:52)
ngd <- ngd[keep]
names(ngd)
names(ngd)[2:ncol(ngd)]
paste0("ngd",1980:2022)
paste0("ngd.",1980:2022)
names(ngd) <- c("iso3c",paste0("ngd.",1980:2022))
names(ngd)
library(reshape)
ngd.long <- reshape(ngd,varying=c(2:ncol(ngd)),direction="long")
View(ngd.long)
dat = read_delim(temp,delim="\t",na=c("n/a","--"),trim_ws=TRUE)
View(dat)
names(dat) <- make.names(names(dat))
ngd <- subset(dat,WEO.Subject.Code=="NGDPRPPPPC")
keep <- c(2,10:52)
ngd <- ngd[keep]
names(ngd) <- c("iso3c",paste0("ngd.",1980:2022))
library(reshape)
ngd.long <- reshape(ngd,varying=c(2:ncol(ngd)),direction="long")
View(ngd.long)
?reshape
ngd.long <- reshape(ngd,varying=c(2:ncol(ngd)),direction="long",timevar="year")
warnings()
View(ngd.long)
library(WDI)
WDI(country="all",indicator="NY.GDP.MKTP.KD.ZG")
WDI(country="all",indicator="NY.GDP.PCAP.CD")
install.packages("WDI")
install.packages("WDI")
dat <- WDI(country="all",indicator="NY.GDP.MKTP.KD.ZG")
library(WDI)
dat <- WDI(country="all",indicator="NY.GDP.MKTP.KD.ZG")
install.packages("wbstats")
library(wbstats)
# Population, total
pop_data <- wb(indicator = "SP.POP.TOTL", startdate = 2000, enddate = 2002)
View(pop_data)
library(WDI)
dat <- WDI(country="all",indicator="SP.POP.TOTL")
library(wbstats)
# Population, total
pop_data <- wb(indicator = "SP.POP.TOTL", startdate = 2000, enddate = 2002)
View(pop_data)
wbsearch(pattern = "gdp growth")
gdp_growth <- wb(indicator= "NY.GDP.MKTP.KD.ZG", startdate=2000,enddate=2002)
gdp_growth <- wb(indicator= "6.0.GDP_growth", startdate=2000,enddate=2002)
View(gdp_growth)
library(jsonlite)
library(foreach)
library(plyr)
byCountry2014 <- fromJSON("http://www.d-portal.org/q?from=act%2Ctrans%2Ccountry&limit=-1&select=country_code%2Csum_of_percent_of_trans_usd&groupby=country_code&trans_code=D%7CE&trans_day_gteq=2014-01-01&trans_day_lt=2015-01-01&reporting_ref=GB-1&view=publisher_countries&_=1441812516302")[[1]]
install.packages("foreach")
byCountry2014 <- fromJSON("http://www.d-portal.org/q?from=act%2Ctrans%2Ccountry&limit=-1&select=country_code%2Csum_of_percent_of_trans_usd&groupby=country_code&trans_code=D%7CE&trans_day_gteq=2014-01-01&trans_day_lt=2015-01-01&reporting_ref=GB-1&view=publisher_countries&_=1441812516302")[[1]]
german2014 <- fromJSON("http://www.d-portal.org/q?from=act%2Ctrans%2Ccountry&limit=-1&select=country_code%2Csum_of_percent_of_trans_usd&groupby=country_code&trans_code=D%7CE&trans_day_gteq=2014-01-01&trans_day_lt=2015-01-01&reporting_ref=DE-1&view=publisher_countries&_=1441812516302")[[1]]
usa2014 <- fromJSON("http://www.d-portal.org/q?from=act%2Ctrans%2Ccountry&limit=-1&select=country_code%2Csum_of_percent_of_trans_usd&groupby=country_code&trans_code=D%7CE&trans_day_gteq=2014-01-01&trans_day_lt=2015-01-01&reporting_ref=US&view=publisher_countries&_=1441812516302")[[1]]
library(jsonlite)
library(foreach)
gbs <- c(
"GB-1"
,"GB-3"
,"GB-4"
,"GB-6"
,"GB-7"
,"GB-9"
,"GB-10"
,"XM-DAC-12-22"
,"GB-GOV-8"
,"GB-COH-RC000346"
)
empty <- fromJSON("http://www.d-portal.org/q?from=act%2Ctrans%2Csector&limit=-1&select=sector_code%2Csum_of_percent_of_trans_usd&groupby=sector_code&trans_code=D%7CE&trans_day_gteq=2014-01-01&trans_day_lt=2015-01-01&reporting_ref=GB-1&view=publisher_sectors&_=1441789807497")[[1]][0,]
empty$year <- double(0)
empty$donor <- character(0)
#Necessary libraries
library(RCurl)
library(rjson)
####OECD SDMX-JSON function####
OECD <- function(url){
content <- getURL(url, httpheader = list('User-Agent' = 'rsdmx-json/0.0.1'), ssl.verifypeer = FALSE, .encoding = "UTF-8")
BOM <- "\ufeff"
if(attr(regexpr(BOM, content), "match.length") != - 1){
content <- gsub(BOM, "", content)
}
rawJson <- fromJSON(content)
rawData <- rawJson$dataSets[[1]]$observations
rawStructure <- rawJson$structure
dimensions <- rawStructure$dimensions[[1]]
attributes <- rawStructure$attributes$observation
names <- c(sapply(dimensions, "[[", 2),"obsValue",sapply(attributes, "[[", 1))
ndim <- length(sapply(dimensions, "[[", 2))
natt <- 1+length(sapply(attributes, "[[", 1))
ncol <- ndim+natt
#data <- setNames(data.frame(matrix(ncol = ncol, nrow = 0)),names)
data <- matrix(ncol=ncol,nrow=length(rawData))
for(i in 1:length(rawData)){
row <- rawData[i]
rawDimensions <- names(row)
splitDimensions <- strsplit(rawDimensions,":")[[1]]
for(j in 1:length(splitDimensions)){
dimensionReference <- dimensions[[j]]$values
dimensionIndex <- as.integer(splitDimensions[j])+1
dimensionValue <- dimensionReference[[dimensionIndex]][[2]]
data[i,j] <- dimensionValue
}
for(j in 1:length(row[[1]])){
if(j>1){
attributeReference <- attributes[[j-1]]$values
rawAttIndex <- row[[1]][[j]]
if(is.null(rawAttIndex)){
attributeValue <- NA
}else{
attributeIndex <- as.integer(rawAttIndex)+1
attributeValue <- attributeReference[[attributeIndex]][[2]]
}
}else{
attributeValue <- as.double(row[[1]][[j]])
}
data[i,ndim+j] <- attributeValue
}
}
data <- setNames(data.frame(data,stringsAsFactors=FALSE),names)
names(data)[which(names(data)=="Year")] <- "obsTime"
return(data)
}
####Table 1 Example####
url <- "http://stats.oecd.org/SDMX-JSON/data/TABLE1/20005+20001+801+1+2+301+68+3+18+4+5+40+20+21+6+701+742+22+7+820+8+76+9.1.5+1010+2102.1121+1122.A+D+N/all?startTime=2005&endTime=2014&dimensionAtObservation=allDimensions"
data <- OECD(url)
View(data)
url <- "http://stats.oecd.org/SDMX-JSON/data/TABLE2A/10200+10100+10010+71+86+64+62+30+66+35+57+45+93+65+63+61+88+55+85+89+10001+10002+130+142+133+136+139+189+10003+225+236+227+287+228+230+229+231+232+233+234+247+235+274+237+245+271+238+239+240+241+243+244+248+249+251+252+253+255+256+257+258+259+275+260+261+266+276+268+269+270+272+273+218+279+278+280+282+283+285+288+265+289+298+10004+10005+376+377+373+328+329+352+331+388+386+336+338+378+340+342+381+347+349+351+354+358+385+361+364+366+382+383+384+375+387+380+389+10006+425+428+431+434+437+440+446+451+454+457+460+463+489+498+10007+10008+725+728+730+740+735+738+742+745+748+751+752+753+755+761+732+764+765+769+789+10009+625+610+611+666+630+612+645+650+613+614+655+635+660+665+640+615+616+617+619+679+689+10011+530+540+543+546+549+552+555+558+561+566+573+576+550+580+589+798+10012+831+832+840+836+859+860+845+850+856+858+861+862+880+866+868+870+872+854+876+889+9998+10016+225+236+287+228+231+232+233+235+274+245+271+238+240+243+244+249+251+252+253+255+256+259+260+266+268+269+272+273+279+278+282+283+285+288+349+728+745+765+625+666+630+635+660+580+836+880+866+872+854+10017+248+279+265+740+614+615+10018+57+93+85+142+136+230+229+234+247+241+261+280+352+342+347+351+364+428+446+451+738+753+755+769+610+612+645+614+665+640+616+617+543+573+550+832+859+860+862+880+868+870+10019+71+86+64+66+65+63+55+130+133+139+227+239+257+275+276+270+218+376+377+352+336+338+378+340+381+354+358+385+366+382+383+384+425+431+434+437+440+454+457+460+463+730+751+764+611+613+655+616+540+543+549+555+831+832+859+845+856+861+870+876+10025+62+30+35+45+61+258+376+373+328+329+331+388+386+361+382+375+387+725+735+742+748+761+732+530+546+552+558+561+566+576+840+850+858+105+10024+88+89+189+237+289+298+380+389+489+498+752+789+650+619+679+689+589+798+889+9998+10030+236+287+228+229+231+232+233+234+247+235+271+238+240+241+243+244+251+252+253+255+256+259+260+266+268+269+272+273+278+282+283+285+288+349+351+364+428+446+625+614+10201+66+93+227+287+228+231+232+238+249+253+255+260+266+279+280+285+288+265+428+451+745+753+625+610+611+630+613+614+660+615+616+617+10202+230+233+244+257+268+270+376+377+328+329+352+338+378+340+381+349+354+385+382+383+384+375+446+457+761+765+655+831+832+836+859+860+845+856+861+862+880+866+870+872+854+10203+64+57+142+133+225+287+228+229+231+232+233+234+247+235+271+238+243+244+248+251+252+253+255+256+260+261+266+272+273+279+278+283+285+265+349+740+765+625+666+635+660+665+640+543+573+550+580+836+859+860+866+872+10150+913+914+916+915+910+906+917+918+919+901+905+904+909+912+988+903+907+902+927+989+816+975+900+959+974+967+963+964+966+10013+71+72+68+82+75+83+84+76+77+69+101+10014+86+93+85+610+611+612+613+614+615+616+617+87+102+10023+71+86+93+85+610+611+612+613+614+615+616+617+72+68+82+75+83+84+76+77+69+101+87+102+79+10040+225+236+227+287+228+230+229+231+232+233+234+247+235+274+245+271+238+239+240+241+243+244+248+249+251+252+253+255+256+257+259+275+260+261+266+268+269+270+272+273+278+280+282+283+285+288+265+377+328+329+352+378+340+381+349+354+382+383+384+375+446+457+832+836+862+880+866+870+872+854+10041+287+230+232+240+244+255+256+260+269+10152+990+996+98+878+1106+10026+10027.20005+20001+801+1+2+301+68+3+18+4+5+40+20+21+6+701+742+22+7+820+8+76+9+69+61+50+10+11+12+302+20002+1012+913+914+921+916+953+906+1011+1013+990+918+932+1311+811+1313+1312+944+901+905+912+988+903+958+976+812+104+951+978+971+959+948+974+967+963+923+964+960+966+928+20018+20006+72+62+30+82+75+546+613+552+83+70+84+45+77+87+566+732+764+55+576+20007+21600+1601+20003+301+4+5+6+701+12+302+20011+1+2+68+3+18+4+5+40+21+6+22+7+76+9+69+61+50+10+12+20004+1+2+68+3+18+4+5+40+21+6+22+7+76+9+69+61+50+10+12+918.1.216.A+D/all?dimensionAtObservation=allDimensions&pid=baa5e675-3487-422e-bb2a-4da97dbbfd4d&startTime=2000&endTime=2014"
data <- OECD(url)
url <- "http://stats.oecd.org/SDMX-JSON/data/TABLE1/20005+20001+801+1+2+301+68+3+18+4+5+40+20+21+6+701+742+22+7+820+8+76+9.1.5+1010+2102.1121+1122.A+D+N/all?startTime=2005&endTime=2014&dimensionAtObservation=allDimensions"
data <- OECD(url)
View(data)
library(readr)
library(Hmisc)
library(data.table)
library(openxlsx)
wd <- "C:/Users/Alex/Documents/Data/IATI/"
setwd(wd)
# iati_unsplit <- read_csv("iati_unsplit.csv")
#
# iati_unsplit <- subset(iati_unsplit,
#   (!is.na(sector_code))
#   & (!is.na(recipient_code))
# )
#
# iati_unsplit_sum <- sum(iati_unsplit$value,na.rm=TRUE)
iati <- read_csv("iati.csv")
iati <- subset(iati,
(!is.na(sector_code))
& (!is.na(recipient_code))
)
# iati_sum <- sum(iati$value,na.rm=TRUE)
#
# diff <- iati_unsplit_sum - iati_sum
#
# perc.diff <- (diff/iati_unsplit_sum)*100
# message(perc.diff)
setwd("C:/git/iati_flat_testing/output/")
wb <- createWorkbook()
for(varname in names(iati)){
if(varname!="category"){
if(nrow(unique(iati[,varname]))<1000){
message(varname)
df <- data.frame(table(iati[,varname],useNA="ifany"))
total <- sum(df$Freq)
df <- transform(df,Perc=Freq/total)
setnames(df,"Var1",varname)
addWorksheet(wb,varname)
writeDataTable(wb,varname,df)
}
}
}
saveWorkbook(wb,file="iati_freq.xlsx",overwrite=TRUE)
rand.samp <- data.table(iati)[sample(.N,5000)]
write.csv(rand.samp,"sample5000.csv",row.names=FALSE,na="")
uni_recip <- data.table(iati)[,.(count=sum(!is.na(iati_identifier))),by=.(publisher,recipient_code)]
write.csv(uni_recip,"recip_pub_tab.csv",row.names=FALSE,na="")
sink("iati_sumstats.txt")
describe(iati)
closeAllConnections()
